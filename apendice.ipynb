{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c5d6d-f452-42ac-80e5-ce1b989ca305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from kor.nodes import Object, Text\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c8e25-3c38-44b6-8de0-a1e47536e048",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bill_cleaner(path):\n",
    "\n",
    "    \"\"\"\n",
    "    Función que devuelve el texto procesado de una factura.\n",
    "\n",
    "    Input:\n",
    "        - path(str): Ruta de la factura.pdf\n",
    "    \n",
    "    Output:\n",
    "        - texto_limpio (str)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    factura = PdfReader(path)\n",
    "    \n",
    "    texto_factura = \"\"\n",
    "    for pagina in factura.pages:\n",
    "        texto_factura += pagina.extract_text()\n",
    "    \n",
    "    # Elimino hiperlinks:\n",
    "    texto_limpio = re.sub(r'\\b(?:http://|https://|www\\.)?\\S+(?:-|\\s)?\\S*?(?:\\.com|\\.es)\\b', \"\", texto_factura).strip()\n",
    "    \n",
    "    # Elimino conjuntos de puntos mayores a 1:\n",
    "    texto_limpio = re.sub(r'\\.{2,}', \"\", texto_limpio).strip()\n",
    "    \n",
    "    # Elimino espacios multiples en blanco y saltos de linea:\n",
    "    texto_limpio = re.sub(r\"\\s+\", \" \", texto_limpio)\n",
    "\n",
    "    return texto_limpio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ea83c-e329-4809-b78c-39c99bac9e2e",
   "metadata": {},
   "source": [
    "# 1. Guardo el schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64772c3a-da64-4dc8-9112-5af9188b740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input = \"data\\\\factura_800.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab6271-73e9-4901-b9d3-661a3d240fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = bill_cleaner(path_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da936a-02d8-4a75-be77-26049db30605",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Object(\n",
    "    id=\"informacion_factura\",\n",
    "    description=\"Informacion del recibo de la luz de una compañia electrica de un determinado cliente\",\n",
    "    attributes=[\n",
    "        Text(\n",
    "            id=\"nombre_cliente\",\n",
    "            description=\"El nombre y los apellidos del cliente\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"dni_cliente\",\n",
    "            description=\"El documento de identificacion fiscal del cliente\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"calle_cliente\",\n",
    "            description=\"La direccion de la calle del cliente\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"cp_cliente\",\n",
    "            description=\"El codigo postal del cliente\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"población_cliente\",\n",
    "            description=\"La poblacion en la que vive el cliente\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"provincia_cliente\",\n",
    "            description=\"La provincia en la que vive el cliente\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"nombre_comercializadora\",\n",
    "            description=\"Nombre de la comercializadora electrica\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"cif_comercializadora\",\n",
    "            description=\"El codigo de identificacion fiscal de la comercializadora electrica\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"dirección_comercializadora\",\n",
    "            description=\"La direccion de la comercializadora electrica\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"cp_comercializadora\",\n",
    "            description=\"El codigo postal de la comercializadora electrica\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"población_comercializadora\",\n",
    "            description=\"La poblacion de la comercializadora electrica\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"provincia_comercializadora\",\n",
    "            description=\"La provincia de la comercializadora electrica\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"número_factura\",\n",
    "            description=\"El numero asociado a la factura\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"inicio_periodo\",\n",
    "            description=\"El inicio del periodo de consumo\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"fin_periodo\",\n",
    "            description=\"El fin del periodo de consumo\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"importe_factura\",\n",
    "            description=\"El importe total de la factura electrica, utilizando ',' para separar los decimales\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"fecha_cargo\",\n",
    "            description=\"La fecha de cobro del importe de la factura electrica\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"consumo_periodo\",\n",
    "            description=\"El consumo del periodo, utilizando ',' para separar los decimales\",\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"potencia_contratada\",\n",
    "            description=\"La potencia contratada por el cliente, utilizando ',' para separar los decimales\",\n",
    "        ),\n",
    "\n",
    "    ],\n",
    "    examples=[\n",
    "        (\n",
    "            input,\n",
    "            [\n",
    "                {'nombre_cliente': 'JERÓNIMO URIARTE IZQUIERDO', 'dni_cliente': '94985339P', 'calle_cliente': 'Camino de Valdemanco', 'cp_cliente': '10183', 'población_cliente': 'Torrequemada', 'provincia_cliente': 'Cáceres', 'nombre_comercializadora': 'SECOM CENTRAL DE COMPRAS SOCIEDAD LIMITADA', 'cif_comercializadora': 'B40605925', 'dirección_comercializadora': 'PEDRO ITURRALDE OCHOA 11', 'cp_comercializadora': '46900', 'población_comercializadora': 'TORRENT', 'provincia_comercializadora': 'VALENCIA', 'número_factura': 'RQ4694566687', 'inicio_periodo': '27.10.2014', 'fin_periodo': '26.12.2014', 'importe_factura': '253,54', 'fecha_cargo': '31.12.2014', 'consumo_periodo': '796', 'potencia_contratada': '3,310'}\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    "    many=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "43d931d4-ebe3-414c-bf3f-d9e2417f0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/schema.pkl', 'wb') as f:\n",
    "    pickle.dump(schema, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064e01c-8afd-4a71-b52b-0a95ee0645cc",
   "metadata": {},
   "source": [
    "# 2. Prueba con Llama3-8b local.\n",
    "\n",
    "Sin buenos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c3790-7f44-4a0a-ab99-6c39c53b3a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"llama3\",\n",
    "                 base_url=\"http://localhost:11434/v1\",\n",
    "                 openai_api_key= api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287d72c-32ae-4283-ba21-f650ca25d5aa",
   "metadata": {},
   "source": [
    "# 3. Preparacion datos para finetuning Llama3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505220c-5a28-400d-bdc6-f22e740de2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\\\\factura_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d920adc-7f8a-462d-863b-7cf4d99b8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_finetuning = []\n",
    "\n",
    "\n",
    "for i in tqdm.tqdm(range(1000)):\n",
    "    # Output\n",
    "    with open(data_path + f\"{i}.json\", 'r', encoding='utf-8') as archivo:\n",
    "        factura_i = json.load(archivo)\n",
    "\n",
    "    # Input\n",
    "    factura = PdfReader(data_path + f\"{i}.pdf\")\n",
    "    \n",
    "    texto_factura = \"\"\n",
    "    for page in factura.pages:\n",
    "        texto_factura += page.extract_text()\n",
    "        \n",
    "    # Elimino saltos de linea\n",
    "    texto_limpio = re.sub(r\"\\s+\", \" \", texto_factura).strip()    \n",
    "    # Elimino puntos\n",
    "    texto_limpio = re.sub(r\"\\.+\", \"\", texto_limpio)    \n",
    "    # Elimino espacios multiples\n",
    "    texto_limpio = re.sub(r\"\\s+\", \" \", texto_limpio)\n",
    "\n",
    "    output = factura_i\n",
    "    input = texto_limpio\n",
    "    instruction = \"Extrae la siguiente informacion en formato JSON de la factura electrica proporcionada: nombre del cliente, dni del cliente, calle del cliente, codigo postal del cliente, poblacion del cliente, provincia del cliente, nombre de la comercializadora, codigo de identificacion fiscal de la comercializadora, direccion de la comercializadora, codigo postal de la comercializadora, la poblacion de la comercializadora, la provincia de la comercializadora, el numero de factura, el inicio del periodo de consumo, el fin del periodo de consumo, el importe total de la factura, la fecha de cargo, el consumo del periodo y la potencia contratada.\"\n",
    "\n",
    "    info_finetuning.append([output, input, instruction])\n",
    "\n",
    "df_finetuning = pd.DataFrame(info_finetuning, columns= [\"output\", \"input\", \"instruction\"])\n",
    "df_finetuning.to_csv(\"df_finetuning.csv\", sep= \",\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "059f7931-7bab-417d-a7b2-14700e754c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'nombre_cliente': 'Conrado Daniel Iglesias', ...</td>\n",
       "      <td>DATOS DE LA FACTURA Nº factura: SV5043664894 R...</td>\n",
       "      <td>Extrae la siguiente informacion en formato JSO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'nombre_cliente': 'LEONARDA JARAMILLO BÁEZ', ...</td>\n",
       "      <td>Lunes a sábado, de 8 a 22 horas Contratación P...</td>\n",
       "      <td>Extrae la siguiente informacion en formato JSO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'nombre_cliente': 'BENEDICTA GALLEGOS AGUILAR...</td>\n",
       "      <td>DATOS DE LA FACTURA Nº factura: H4623704265 Re...</td>\n",
       "      <td>Extrae la siguiente informacion en formato JSO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'nombre_cliente': 'Belinda Zetina Mijares', '...</td>\n",
       "      <td>DATOS DE LA FACTURA Nº factura: SF3956122542 R...</td>\n",
       "      <td>Extrae la siguiente informacion en formato JSO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'nombre_cliente': 'PANTALEÓN VELASCO DE ALBA'...</td>\n",
       "      <td>DATOS DE LA FACTURA IMPORTE FACTURA: 61,84 € N...</td>\n",
       "      <td>Extrae la siguiente informacion en formato JSO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>{'nombre_cliente': 'SULPICIO ESCOVAR FONSECA',...</td>\n",
       "      <td>Página 1 / 2 ELECTRICA NTRA SRA DE GRACIA SDAD...</td>\n",
       "      <td>Extrae la siguiente informacion en formato JSO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>{'nombre_cliente': 'Petrona Uribe Naranjo', 'd...</td>\n",
       "      <td>DATOS DE LA FACTURA Nº factura: U2093855017 Re...</td>\n",
       "      <td>Extrae la siguiente informacion en formato JSO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>{'nombre_cliente': 'CELESTINA TREMINIO VALLEJO...</td>\n",
       "      <td>DATOS DE LA FACTURA IMPORTE FACTURA: 29,30 € N...</td>\n",
       "      <td>Extrae la siguiente informacion en formato JSO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>{'nombre_cliente': 'Dela Anaya Naranjo', 'dni_...</td>\n",
       "      <td>Página 1 / 2 ENERGÉTICA DEL ESTE SL CIF B40563...</td>\n",
       "      <td>Extrae la siguiente informacion en formato JSO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>{'nombre_cliente': 'KIRA GAITÁN QUESADA', 'dni...</td>\n",
       "      <td>DATOS DE LA FACTURA Nº factura: BD9767921453 R...</td>\n",
       "      <td>Extrae la siguiente informacion en formato JSO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                output  \\\n",
       "0    {'nombre_cliente': 'Conrado Daniel Iglesias', ...   \n",
       "1    {'nombre_cliente': 'LEONARDA JARAMILLO BÁEZ', ...   \n",
       "2    {'nombre_cliente': 'BENEDICTA GALLEGOS AGUILAR...   \n",
       "3    {'nombre_cliente': 'Belinda Zetina Mijares', '...   \n",
       "4    {'nombre_cliente': 'PANTALEÓN VELASCO DE ALBA'...   \n",
       "..                                                 ...   \n",
       "995  {'nombre_cliente': 'SULPICIO ESCOVAR FONSECA',...   \n",
       "996  {'nombre_cliente': 'Petrona Uribe Naranjo', 'd...   \n",
       "997  {'nombre_cliente': 'CELESTINA TREMINIO VALLEJO...   \n",
       "998  {'nombre_cliente': 'Dela Anaya Naranjo', 'dni_...   \n",
       "999  {'nombre_cliente': 'KIRA GAITÁN QUESADA', 'dni...   \n",
       "\n",
       "                                                 input  \\\n",
       "0    DATOS DE LA FACTURA Nº factura: SV5043664894 R...   \n",
       "1    Lunes a sábado, de 8 a 22 horas Contratación P...   \n",
       "2    DATOS DE LA FACTURA Nº factura: H4623704265 Re...   \n",
       "3    DATOS DE LA FACTURA Nº factura: SF3956122542 R...   \n",
       "4    DATOS DE LA FACTURA IMPORTE FACTURA: 61,84 € N...   \n",
       "..                                                 ...   \n",
       "995  Página 1 / 2 ELECTRICA NTRA SRA DE GRACIA SDAD...   \n",
       "996  DATOS DE LA FACTURA Nº factura: U2093855017 Re...   \n",
       "997  DATOS DE LA FACTURA IMPORTE FACTURA: 29,30 € N...   \n",
       "998  Página 1 / 2 ENERGÉTICA DEL ESTE SL CIF B40563...   \n",
       "999  DATOS DE LA FACTURA Nº factura: BD9767921453 R...   \n",
       "\n",
       "                                           instruction  \n",
       "0    Extrae la siguiente informacion en formato JSO...  \n",
       "1    Extrae la siguiente informacion en formato JSO...  \n",
       "2    Extrae la siguiente informacion en formato JSO...  \n",
       "3    Extrae la siguiente informacion en formato JSO...  \n",
       "4    Extrae la siguiente informacion en formato JSO...  \n",
       "..                                                 ...  \n",
       "995  Extrae la siguiente informacion en formato JSO...  \n",
       "996  Extrae la siguiente informacion en formato JSO...  \n",
       "997  Extrae la siguiente informacion en formato JSO...  \n",
       "998  Extrae la siguiente informacion en formato JSO...  \n",
       "999  Extrae la siguiente informacion en formato JSO...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed09cf5-6d39-4e85-bad4-d96ab49c3233",
   "metadata": {
    "id": "evUj2Wok-qz6"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc322958-cc97-46e7-b4a8-2d7f4759085f",
   "metadata": {
    "id": "jMzO-bcPciaN"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5356db6-0607-4cc6-8833-7ec60d37496b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "e8364ab6-e897-4312-e249-1be76542f92d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c57fd32-8395-4e61-a3eb-99d6742a49da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVoqoA9uuhuK",
    "outputId": "67a3095c-b578-40d2-f993-6c2dc357ac8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: galore_torch in /usr/local/lib/python3.10/dist-packages (1.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from galore_torch) (2.2.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from galore_torch) (4.40.1)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from galore_torch) (0.43.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes->galore_torch) (1.25.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->galore_torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->galore_torch) (12.4.127)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->galore_torch) (0.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->galore_torch) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->galore_torch) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->galore_torch) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->galore_torch) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->galore_torch) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->galore_torch) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->galore_torch) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->galore_torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->galore_torch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->galore_torch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->galore_torch) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->galore_torch) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->galore_torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install galore_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135c007-83a1-422d-a6d3-595009e54833",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWki3UKNIKu9",
    "outputId": "e929e1d9-d09d-4f47-91b3-a3ac6a611671"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df2a8ec-5bd1-4ef3-9399-e06df9e745b4",
   "metadata": {},
   "source": [
    "## Preparo el prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7546c874-c3f1-4ebf-9456-57b00f8aac20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5f13468-638d-4ba8-b10a-73c0a02ed850",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef0312-f4a3-405e-9c73-c3489135c69a",
   "metadata": {
    "id": "ozeF7H_ysHUL"
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc4a1c-ef87-4e2f-ba93-5222106dc630",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "EvLUBcaoAbmD",
    "outputId": "36333ebc-3f25-482f-85ba-4d7507870d25"
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0be852-ba45-46ca-93fa-993bfae5a04e",
   "metadata": {
    "id": "y77z8Mi2GBYV"
   },
   "outputs": [],
   "source": [
    "train = df_finetuning[:800]\n",
    "validation = df_finetuning[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a9557b-3924-4b85-afcc-814a722dc8cc",
   "metadata": {
    "id": "mz3p12OTDomh"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset_finetuning = Dataset.from_pandas(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52be76-4e67-4866-b528-5c4dedf23099",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ef6c0578f269473d8a150c2445aa4e27",
      "6dc757c27129447ebdd506caf31a1a8b",
      "b65edced35e34fb19e25428195fad69e",
      "cd4b59dea10a4edab86466d731019de8",
      "29a0daae462e4589adc6537d318da6ef",
      "c06301cc9aaf4f15953c662691dbb17c",
      "a07460f55fc24cd88f63ca710e4c4f70",
      "81fdd8738cb348a2806852aa8a0d5008",
      "71205c93fb6744af9654ea51e707101f",
      "f3dfa1c2a0704ff38d9528615928ed41",
      "c949a8177d124dd894f98cf6fb092b06"
     ]
    },
    "id": "QtEKZBlQBALj",
    "outputId": "80fd6932-3443-4534-c29b-f9719e385efa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6c0578f269473d8a150c2445aa4e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_finetuning = dataset_finetuning.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d6fb4-0f78-4d97-8a5d-07fb1f68a798",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkiaiUjlvr3Y",
    "outputId": "03d5320a-4999-4cdf-9e80-38d5a6e0877b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mod  base_model.model.model.layers.0.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.0.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.0.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.0.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.0.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.0.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.0.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.0.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.0.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.0.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.0.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.0.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.0.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.0.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.0.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.0.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.0.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.0.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.0.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.0.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.0.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.1.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.1.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.1.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.1.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.1.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.1.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.1.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.1.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.1.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.1.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.1.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.1.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.1.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.1.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.1.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.1.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.1.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.1.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.1.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.1.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.1.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.2.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.2.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.2.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.2.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.2.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.2.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.2.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.2.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.2.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.2.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.2.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.2.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.2.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.2.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.2.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.2.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.2.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.2.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.2.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.2.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.2.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.3.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.3.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.3.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.3.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.3.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.3.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.3.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.3.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.3.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.3.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.3.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.3.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.3.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.3.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.3.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.3.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.3.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.3.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.3.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.3.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.3.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.4.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.4.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.4.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.4.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.4.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.4.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.4.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.4.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.4.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.4.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.4.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.4.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.4.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.4.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.4.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.4.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.4.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.4.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.4.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.4.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.4.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.5.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.5.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.5.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.5.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.5.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.5.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.5.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.5.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.5.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.5.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.5.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.5.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.5.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.5.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.5.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.5.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.5.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.5.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.5.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.5.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.5.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.6.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.6.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.6.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.6.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.6.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.6.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.6.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.6.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.6.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.6.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.6.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.6.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.6.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.6.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.6.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.6.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.6.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.6.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.6.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.6.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.6.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.7.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.7.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.7.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.7.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.7.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.7.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.7.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.7.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.7.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.7.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.7.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.7.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.7.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.7.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.7.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.7.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.7.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.7.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.7.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.7.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.7.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.8.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.8.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.8.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.8.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.8.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.8.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.8.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.8.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.8.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.8.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.8.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.8.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.8.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.8.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.8.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.8.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.8.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.8.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.8.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.8.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.8.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.9.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.9.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.9.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.9.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.9.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.9.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.9.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.9.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.9.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.9.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.9.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.9.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.9.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.9.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.9.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.9.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.9.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.9.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.9.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.9.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.9.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.10.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.10.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.10.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.10.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.10.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.10.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.10.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.10.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.10.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.10.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.10.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.10.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.10.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.10.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.10.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.10.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.10.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.10.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.10.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.10.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.10.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.11.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.11.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.11.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.11.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.11.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.11.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.11.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.11.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.11.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.11.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.11.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.11.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.11.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.11.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.11.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.11.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.11.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.11.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.11.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.11.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.11.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.12.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.12.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.12.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.12.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.12.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.12.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.12.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.12.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.12.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.12.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.12.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.12.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.12.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.12.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.12.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.12.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.12.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.12.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.12.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.12.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.12.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.13.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.13.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.13.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.13.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.13.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.13.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.13.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.13.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.13.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.13.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.13.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.13.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.13.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.13.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.13.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.13.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.13.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.13.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.13.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.13.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.13.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.14.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.14.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.14.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.14.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.14.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.14.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.14.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.14.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.14.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.14.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.14.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.14.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.14.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.14.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.14.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.14.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.14.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.14.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.14.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.14.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.14.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.15.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.15.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.15.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.15.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.15.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.15.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.15.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.15.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.15.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.15.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.15.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.15.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.15.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.15.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.15.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.15.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.15.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.15.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.15.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.15.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.15.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.16.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.16.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.16.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.16.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.16.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.16.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.16.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.16.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.16.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.16.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.16.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.16.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.16.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.16.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.16.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.16.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.16.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.16.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.16.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.16.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.16.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.17.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.17.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.17.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.17.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.17.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.17.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.17.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.17.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.17.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.17.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.17.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.17.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.17.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.17.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.17.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.17.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.17.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.17.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.17.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.17.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.17.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.18.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.18.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.18.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.18.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.18.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.18.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.18.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.18.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.18.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.18.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.18.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.18.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.18.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.18.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.18.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.18.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.18.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.18.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.18.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.18.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.18.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.19.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.19.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.19.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.19.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.19.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.19.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.19.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.19.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.19.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.19.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.19.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.19.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.19.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.19.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.19.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.19.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.19.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.19.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.19.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.19.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.19.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.20.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.20.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.20.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.20.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.20.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.20.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.20.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.20.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.20.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.20.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.20.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.20.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.20.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.20.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.20.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.20.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.20.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.20.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.20.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.20.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.20.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.21.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.21.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.21.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.21.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.21.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.21.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.21.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.21.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.21.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.21.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.21.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.21.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.21.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.21.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.21.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.21.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.21.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.21.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.21.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.21.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.21.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.22.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.22.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.22.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.22.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.22.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.22.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.22.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.22.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.22.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.22.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.22.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.22.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.22.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.22.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.22.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.22.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.22.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.22.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.22.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.22.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.22.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.23.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.23.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.23.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.23.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.23.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.23.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.23.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.23.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.23.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.23.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.23.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.23.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.23.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.23.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.23.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.23.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.23.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.23.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.23.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.23.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.23.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.24.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.24.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.24.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.24.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.24.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.24.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.24.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.24.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.24.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.24.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.24.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.24.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.24.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.24.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.24.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.24.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.24.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.24.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.24.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.24.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.24.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.25.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.25.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.25.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.25.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.25.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.25.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.25.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.25.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.25.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.25.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.25.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.25.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.25.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.25.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.25.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.25.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.25.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.25.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.25.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.25.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.25.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.26.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.26.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.26.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.26.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.26.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.26.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.26.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.26.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.26.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.26.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.26.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.26.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.26.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.26.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.26.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.26.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.26.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.26.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.26.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.26.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.26.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.27.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.27.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.27.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.27.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.27.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.27.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.27.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.27.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.27.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.27.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.27.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.27.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.27.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.27.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.27.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.27.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.27.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.27.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.27.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.27.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.27.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.28.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.28.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.28.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.28.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.28.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.28.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.28.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.28.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.28.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.28.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.28.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.28.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.28.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.28.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.28.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.28.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.28.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.28.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.28.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.28.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.28.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.29.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.29.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.29.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.29.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.29.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.29.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.29.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.29.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.29.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.29.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.29.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.29.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.29.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.29.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.29.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.29.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.29.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.29.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.29.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.29.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.29.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.30.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.30.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.30.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.30.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.30.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.30.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.30.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.30.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.30.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.30.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.30.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.30.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.30.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.30.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.30.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.30.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.30.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.30.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.30.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.30.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.30.mlp.down_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.31.self_attn.q_proj.base_layer\n",
      "mod  base_model.model.model.layers.31.self_attn.q_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.31.self_attn.q_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.31.self_attn.k_proj.base_layer\n",
      "mod  base_model.model.model.layers.31.self_attn.k_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.31.self_attn.k_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.31.self_attn.v_proj.base_layer\n",
      "mod  base_model.model.model.layers.31.self_attn.v_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.31.self_attn.v_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.31.self_attn.o_proj.base_layer\n",
      "mod  base_model.model.model.layers.31.self_attn.o_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.31.self_attn.o_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.31.mlp.gate_proj.base_layer\n",
      "mod  base_model.model.model.layers.31.mlp.gate_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.31.mlp.gate_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.31.mlp.up_proj.base_layer\n",
      "mod  base_model.model.model.layers.31.mlp.up_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.31.mlp.up_proj.lora_B.default\n",
      "mod  base_model.model.model.layers.31.mlp.down_proj.base_layer\n",
      "mod  base_model.model.model.layers.31.mlp.down_proj.lora_A.default\n",
      "mod  base_model.model.model.layers.31.mlp.down_proj.lora_B.default\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from galore_torch import GaLoreAdamW8bit\n",
    "import torch.nn as nn\n",
    "galore_params = []\n",
    "target_modules_list = [\"attn\", \"mlp\"]\n",
    "for module_name, module in model.named_modules():\n",
    "    if not isinstance(module, nn.Linear):\n",
    "        continue\n",
    "\n",
    "    if not any(target_key in module_name for target_key in target_modules_list):\n",
    "        continue\n",
    "\n",
    "    print('mod ', module_name)\n",
    "    galore_params.append(module.weight)\n",
    "id_galore_params = [id(p) for p in galore_params]\n",
    "regular_params = [p for p in model.parameters() if id(p) not in id_galore_params]\n",
    "\n",
    "\n",
    "param_groups = [{'params': regular_params},\n",
    "                {'params': galore_params, 'rank': 64, 'update_proj_gap': 200, 'scale': 0.25, 'proj_type': 'std'}]\n",
    "optimizer = GaLoreAdamW8bit(param_groups, lr=2e-5)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_finetuning,\n",
    "    optimizers=(optimizer, None),\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = True, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4dad8-da59-4b2a-9f0a-83cda33101bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8XdEoy4yyHTF",
    "outputId": "2b1fccec-fc01-4e61-88c4-1b4bf6e9af37",
    "scrolled": true
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 233 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 4 | Total steps = 58\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/58 1:40:19 < 05:40, 0.01 it/s, Epoch 0.93/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.400600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.322500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.555200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.401300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.388600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.402700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.423700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.347800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.299400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.353200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.364400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.392600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.368000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58/58 1:47:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.400600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.322500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.555200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.401300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.388600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.402700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.423700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.347800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.299400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.353200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.364400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.392600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.282100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.252600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a37f60-f5bd-4020-911f-d30e25b76444",
   "metadata": {
    "id": "Bz1qxW30yaR4"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f4c87-db75-4a43-89c6-7d2c04725416",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400,
     "referenced_widgets": [
      "fab46bb1ac3649bcac3296ea3e09a096",
      "5d59c0b4708244afa6a11ba2b9ef244a",
      "118c7d024c1c436d89e415f5a0106df0",
      "ac7d483880d443b7ac84b4a6f7760b9b",
      "8adf51648294499d888aece7b917e0b1",
      "2798eb08ff4544c4a13f7f5ca5273a45",
      "0d985d654b2440bda80cce13ffb17e8e",
      "e93f5b696d6742d9aca2209a1e587caa",
      "5ef138a920e448efaaeb06aa5f250e1d",
      "3034fc9b20264ccf94f274ce53a18b7d",
      "fa594a9fcbd44bfe9acd16f88b66b482",
      "07d4e1483b194ee7984c79674f2ddaec",
      "3f93018283594a0394e77e43db25bbe4",
      "7fb8a2d687a049d4ad1d8deb70d05031",
      "498831bccad8432d8758c666de0a1cc8",
      "cce074cbd81a4e39856123fc6aca2a40",
      "7b1c1bfd8e244d21a420578ae7103509",
      "5cc6363eeab647749196c203a712db92",
      "6f8bcd1a649f4ba4bd061bfb80536a5e",
      "7e171b532d28423daaa3bf6cd3fe65ed",
      "7e4179cb2a7640c58f59fc673101dcbc",
      "fb339f43caed48b39cec8d548e96e803",
      "66daf2006ec44326abb622ce9ab061bf",
      "7fb23af90fdb4e6f857a06fa9f22a09d",
      "ce41c77474014343b70b38e8d6aee2da",
      "f204132dd18148f1bdee160fd986e726",
      "244fec81b393458bb4fe85557464ccda",
      "53a0c50376e44fbb983d0a7af10bcf17",
      "0b1a7d9cd00f466eaf6eff483385ecf2",
      "0e6f3f49944d4900a9853c18b81bc0bf",
      "d99996c690054a4dae0fc216e2854927",
      "a9620df8c2f846d3a5b38e738748ad2e",
      "49a7fe52618845d1b2906a7a163b7e72",
      "b1e6432f243649c2aba14e4251db7ff0",
      "e68182ca54b64f7d8cd4569f061d03b8",
      "24c79e85a5ed4a5aa3bf2ea7b768550e",
      "2731e3cd6a774d538f4200529dbaf571",
      "4617b8fffae4479aa8c8797b56905a92",
      "2474132a72aa4391a886dd7d64027ec4",
      "834e7a3719934ca496677b8eb6f5c232",
      "dcfdea32dea5430b83b0aa666be66210",
      "10ca725d62514982a8918cc73e3c510b",
      "6446b655fc934b3985ed6fd03c354d57",
      "95fa0618103446c99d34005d35e79411",
      "47af5311d0ea44e7b0423f9ef03ce460",
      "78741cc450554bb3aa4ffda7d7058dab",
      "9e20a9b4250841beaf366903cff9baa3",
      "a306dcacd1a6455e9bd62464188215c2",
      "458a94ff9bb446c29bd35bb633c1448e",
      "b55c8e5cc0e44d938b8e581dab750327",
      "23127072a2224221aa137c6e8329d140",
      "470ba92bfacf4b968eb1deb9df8ad7dd",
      "87f862feb68b4ba4a2b7813d221cf1d5",
      "b73417e61c224f6dbf368456563f2497",
      "6692018a8b384414846c17c5bd38c36c",
      "d1abf8b52b0f4cadad7b930e462d4698",
      "4b73f6a4c8644bdcb7eb5da3e0101736",
      "c71e96fd217f409c834f93481181061d",
      "b56d3add6a95418790ce5f8352b5ce08",
      "5a4583e032764c89a9a597dc54b8078a",
      "3b607e31dc7842dcb858b7561f771afe",
      "9edef7f538434c3e817cee733798b6ef",
      "33de8c10267741a3b100faf2666946b8",
      "9104c8547ee14484aaa1386b6bef4440",
      "71ed700fb40e42deae049eee1f0558fd",
      "1bf88ace81544f999d1705e33ba33490"
     ]
    },
    "id": "aKZu_tQhyagm",
    "outputId": "cb1a9e5e-e99d-4861-a985-8d97a463119e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 4.57 out of 12.67 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [03:21<00:00,  6.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Unsloth: Saving finetuned-llama-3-8B/pytorch_model-00001-of-00004.bin...\n",
      "Unsloth: Saving finetuned-llama-3-8B/pytorch_model-00002-of-00004.bin...\n",
      "Unsloth: Saving finetuned-llama-3-8B/pytorch_model-00003-of-00004.bin...\n",
      "Unsloth: Saving finetuned-llama-3-8B/pytorch_model-00004-of-00004.bin...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab46bb1ac3649bcac3296ea3e09a096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/573 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d4e1483b194ee7984c79674f2ddaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00004.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66daf2006ec44326abb622ce9ab061bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e6432f243649c2aba14e4251db7ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00004.bin:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47af5311d0ea44e7b0423f9ef03ce460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00004.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1abf8b52b0f4cadad7b930e462d4698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00004.bin:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/UrkoRR/finetuned-llama-3-8B\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub_merged(\"UrkoRR/finetuned-llama-3-8B\", tokenizer, save_method = \"merged_16bit\", token = hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d34d90-fd18-4edf-8e58-0819ae811aba",
   "metadata": {},
   "source": [
    "### Prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0447bc-eb88-4fe1-8226-421f111031b5",
   "metadata": {
    "id": "jqBkZuvhJB_M"
   },
   "outputs": [],
   "source": [
    "instruction = \"Extrae la siguiente informacion en formato JSON de la factura electrica proporcionada: nombre del cliente, dni del cliente, calle del cliente, codigo postal del cliente, poblacion del cliente, provincia del cliente, nombre de la comercializadora, codigo de identificacion fiscal de la comercializadora, direccion de la comercializadora, codigo postal de la comercializadora, la poblacion de la comercializadora, la provincia de la comercializadora, el numero de factura, el inicio del periodo de consumo, el fin del periodo de consumo, el importe total de la factura, la fecha de cargo, el consumo del periodo y la potencia contratada.\"\n",
    "input = validation.loc[800].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6a219-2542-48ca-b1f1-da66bb439765",
   "metadata": {
    "id": "i3OGN2fTKKGb"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([alpaca_prompt.format(instruction, # instruction\n",
    "                                         input, # input\n",
    "                                         \"\", # output\n",
    "                                        )],\n",
    "                   return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
